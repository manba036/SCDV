{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import codecs\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "dirlist = [\"dokujo-tsushin\",\"it-life-hack\",\"kaden-channel\",\"livedoor-homme\",\n",
    "           \"movie-enter\",\"peachy\",\"smax\",\"sports-watch\",\"topic-news\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"class\",\"news\"])\n",
    "for i in tqdm(dirlist):\n",
    "    path = \"../japanese-dataset/livedoor-news-corpus/\"+i+\"/*.txt\"\n",
    "    files = glob.glob(path)\n",
    "    files.pop()\n",
    "    for j in tqdm(files):\n",
    "        f = codecs.open(j, 'r', 'utf-8')\n",
    "        data = f.read() \n",
    "        f.close()\n",
    "        t = pd.Series([i,\"\".join(data.split(\"\\n\")[3:])],index = df.columns)\n",
    "        df  = df.append(t,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"class\"]==\"livedoor-homme\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../japanese-dataset/livedoor-news-corpus/all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create word2vec\n",
    "import logging\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import MeCab\n",
    "import time\n",
    "from sklearn.preprocessing import normalize\n",
    "import sys\n",
    "import re\n",
    " \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "tokenizer =  MeCab.Tagger(\"-Owakati\")  \n",
    "sentences = []\n",
    "print (\"Parsing sentences from training set...\")\n",
    "\n",
    "# Loop over each news article.\n",
    "for review in tqdm(df[\"news\"]):\n",
    "    try:\n",
    "        # Split a review into parsed sentences.\n",
    "        result = tokenizer.parse(review).replace(\"\\u3000\",\"\").replace(\"\\n\",\"\")\n",
    "        result = re.sub(r'[0123456789０１２３４５６７８９！＠＃＄％＾＆\\-|\\\\＊\\“（）＿■×※⇒—●(：〜＋=)／*&^%$#@!~`){}…\\[\\]\\\"\\'\\”:;<>?＜＞？、。・,./『』【】「」→←○]+', \"\", result)\n",
    "        h = result.split(\" \")\n",
    "        h = list(filter((\"\").__ne__, h))\n",
    "        sentences.append(h)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "num_features = 200     # Word vector dimensionality\n",
    "min_word_count = 20   # Minimum word count\n",
    "num_workers = 40       # Number of threads to run in parallel\n",
    "context = 10          # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "print (\"Training Word2Vec model...\")\n",
    "# Train Word2Vec model.\n",
    "model = Word2Vec(sentences, workers=num_workers, hs = 0, sg = 1, negative = 10, iter = 25,\\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling, seed=1)\n",
    "\n",
    "model_name = str(num_features) + \"features_\" + str(min_word_count) + \"minwords_\" + str(context) + \"context_len2alldata\"\n",
    "model.init_sims(replace=True)\n",
    "# Save Word2Vec model.\n",
    "print (\"Saving Word2Vec model...\")\n",
    "model.save(\"../japanese-dataset/livedoor-news-corpus/model/\"+model_name)\n",
    "endmodeltime = time.time()\n",
    "\n",
    "print (\"time : \", endmodeltime-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plain word2vec t-SNE Visualization\n",
    "word2vec_model=model\n",
    "\n",
    "skip=0\n",
    "limit=241 \n",
    "\n",
    "vocab = word2vec_model.wv.vocab\n",
    "emb_tuple = tuple([word2vec_model[v] for v in vocab])\n",
    "X = np.vstack(emb_tuple)\n",
    " \n",
    "model = TSNE(n_components=2, random_state=0,verbose=2)\n",
    "np.set_printoptions(suppress=True)\n",
    "model.fit_transform(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly\n",
    "# import plotly.plotly as py\n",
    "# import plotly.graph_objs as go\n",
    "# plotly.offline.init_notebook_mode(connected=False)\n",
    "# # Create a trace\n",
    "# trace = go.Scatter(\n",
    "#     x =model.embedding_[skip:limit, 0],\n",
    "#     y = model.embedding_[skip:limit, 1],\n",
    "#     text=list(vocab.keys()),\n",
    "#     mode = 'markers+text'\n",
    "# )\n",
    "\n",
    "# data = [trace]\n",
    "\n",
    "# # Plot and embed in ipython notebook!\n",
    "# plotly.offline.iplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create gwbowv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,HashingVectorizer\n",
    "import pickle\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drange(start, stop, step):\n",
    "    r = start\n",
    "    while r < stop:\n",
    "        yield r\n",
    "        r += step\n",
    "\n",
    "def cluster_GMM(num_clusters, word_vectors):\n",
    "    # Initalize a GMM object and use it for clustering.\n",
    "    clf =  GaussianMixture(n_components=num_clusters,\n",
    "                    covariance_type=\"tied\", init_params='kmeans', max_iter=50)\n",
    "    # Get cluster assignments.\n",
    "    clf.fit(word_vectors)\n",
    "    idx = clf.predict(word_vectors)\n",
    "    print (\"Clustering Done...\", time.time()-start, \"seconds\")\n",
    "    # Get probabilities of cluster assignments.\n",
    "    idx_proba = clf.predict_proba(word_vectors)\n",
    "    # Dump cluster assignments and probability of cluster assignments. \n",
    "    pickle.dump(idx, open('../japanese-dataset/livedoor-news-corpus/model/gmm_latestclusmodel_len2alldata.pkl',\"wb\"))\n",
    "    print (\"Cluster Assignments Saved...\")\n",
    "\n",
    "    pickle.dump(idx_proba,open( '../japanese-dataset/livedoor-news-corpus/model/gmm_prob_latestclusmodel_len2alldata.pkl',\"wb\"))\n",
    "    print (\"Probabilities of Cluster Assignments Saved...\")\n",
    "    return (idx, idx_proba)\n",
    "\n",
    "def read_GMM(idx_name, idx_proba_name):\n",
    "    # Loads cluster assignments and probability of cluster assignments. \n",
    "    idx = pickle.load(open('../japanese-dataset/livedoor-news-corpus/model/gmm_latestclusmodel_len2alldata.pkl',\"rb\"))\n",
    "    idx_proba = pickle.load(open( '../japanese-dataset/livedoor-news-corpus/model/gmm_prob_latestclusmodel_len2alldata.pkl',\"rb\"))\n",
    "    print (\"Cluster Model Loaded...\")\n",
    "    return (idx, idx_proba)\n",
    "\n",
    "def get_probability_word_vectors(featurenames, word_centroid_map, num_clusters, word_idf_dict):\n",
    "    # This function computes probability word-cluster vectors\n",
    "    prob_wordvecs = {}\n",
    "    for word in word_centroid_map:\n",
    "        prob_wordvecs[word] = np.zeros( num_clusters * num_features, dtype=\"float32\" )\n",
    "        for index in range(0, num_clusters):\n",
    "            try:\n",
    "                prob_wordvecs[word][index*num_features:(index+1)*num_features] = model[word] * word_centroid_prob_map[word][index] * word_idf_dict[word]\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    # prob_wordvecs_idf_len2alldata = {}\n",
    "    # i = 0\n",
    "    # for word in featurenames:\n",
    "    #     i += 1\n",
    "    #     if word in word_centroid_map:    \n",
    "    #         prob_wordvecs_idf_len2alldata[word] = {}\n",
    "    #         for index in range(0, num_clusters):\n",
    "    #                 prob_wordvecs_idf_len2alldata[word][index] = model[word] * word_centroid_prob_map[word][index] * word_idf_dict[word] \n",
    "\n",
    "    \n",
    "\n",
    "    # for word in prob_wordvecs_idf_len2alldata.keys():\n",
    "    #     prob_wordvecs[word] = prob_wordvecs_idf_len2alldata[word][0]\n",
    "    #     for index in prob_wordvecs_idf_len2alldata[word].keys():\n",
    "    #         if index==0:\n",
    "    #             continue\n",
    "    #         prob_wordvecs[word] = np.concatenate((prob_wordvecs[word], prob_wordvecs_idf_len2alldata[word][index]))\n",
    "    return prob_wordvecs\n",
    "\n",
    "def create_cluster_vector_and_gwbowv(prob_wordvecs, wordlist, word_centroid_map, word_centroid_prob_map, dimension, word_idf_dict, featurenames, num_centroids, train=False):\n",
    "    # This function computes SDV feature vectors.\n",
    "    bag_of_centroids = np.zeros( num_centroids * dimension, dtype=\"float32\" )\n",
    "    global min_no\n",
    "    global max_no\n",
    "\n",
    "    for word in wordlist:\n",
    "        try:\n",
    "            temp = word_centroid_map[word]\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        bag_of_centroids += prob_wordvecs[word]\n",
    "\n",
    "    norm = np.sqrt(np.einsum('...i,...i', bag_of_centroids, bag_of_centroids))\n",
    "    if(norm!=0):\n",
    "        bag_of_centroids /= norm\n",
    "\n",
    "    # To make feature vector sparse, make note of minimum and maximum values.\n",
    "    if train:\n",
    "        min_no += min(bag_of_centroids)\n",
    "        max_no += max(bag_of_centroids)\n",
    "\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 200     # Word vector dimensionality\n",
    "min_word_count = 20   # Minimum word count\n",
    "num_workers = 40       # Number of threads to run in parallel\n",
    "context = 10          # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "model_name = str(num_features) + \"features_\" + str(min_word_count) + \"minwords_\" + str(context) + \"context_len2alldata\"\n",
    "# Load the trained Word2Vec model.\n",
    "word2vec_model = Word2Vec.load(\"../japanese-dataset/livedoor-news-corpus/model/\"+model_name)\n",
    "# Get wordvectors for all words in vocabulary.\n",
    "word_vectors = word2vec_model.wv.syn0\n",
    "\n",
    "# Load train data.\n",
    "train,test = train_test_split(df,test_size=0.3,random_state=40)\n",
    "all = df\n",
    "\n",
    "# Set number of clusters.\n",
    "#num_clusters = 60\n",
    "num_clusters = 1\n",
    "# Uncomment below line for creating new clusters.\n",
    "idx, idx_proba = cluster_GMM(num_clusters, word_vectors)\n",
    "\n",
    "# Uncomment below lines for loading saved cluster assignments and probabaility of cluster assignments.\n",
    "# idx_name = \"gmm_latestclusmodel_len2alldata.pkl\"\n",
    "# idx_proba_name = \"gmm_prob_latestclusmodel_len2alldata.pkl\"\n",
    "# idx, idx_proba = read_GMM(idx_name, idx_proba_name)\n",
    "\n",
    "# Create a Word / Index dictionary, mapping each vocabulary word to\n",
    "# a cluster number\n",
    "word_centroid_map = dict(zip( word2vec_model.wv.index2word, idx ))\n",
    "# Create a Word / Probability of cluster assignment dictionary, mapping each vocabulary word to\n",
    "# list of probabilities of cluster assignments.\n",
    "word_centroid_prob_map = dict(zip( word2vec_model.wv.index2word, idx_proba ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing tf-idf values.\n",
    "traindata = []\n",
    "for review in all[\"news\"]:\n",
    "    result = tokenizer.parse(review).replace(\"\\u3000\",\"\").replace(\"\\n\",\"\")\n",
    "    result = re.sub(r'[0123456789０１２３４５６７８９！＠＃＄％＾＆\\-|\\\\＊\\“（）＿■×※⇒—●(：〜＋=)／*&^%$#@!~`){}…\\[\\]\\\"\\'\\”:;<>?＜＞？、。・,./『』【】「」→←○]+', \"\", result)\n",
    "    h = result.split(\" \")\n",
    "    h = filter((\"\").__ne__, h)\n",
    "    traindata.append(\" \".join(h))\n",
    "\n",
    "tfv = TfidfVectorizer(dtype=np.float32)\n",
    "tfidfmatrix_traindata = tfv.fit_transform(traindata)\n",
    "featurenames = tfv.get_feature_names()\n",
    "idf = tfv._tfidf.idf_\n",
    "\n",
    "# Creating a dictionary with word mapped to its idf value \n",
    "print (\"Creating word-idf dictionary for Training set...\")\n",
    "\n",
    "word_idf_dict = {}\n",
    "for pair in zip(featurenames, idf):\n",
    "    word_idf_dict[pair[0]] = pair[1]\n",
    "    \n",
    "# Pre-computing probability word-cluster vectors.\n",
    "prob_wordvecs = get_probability_word_vectors(featurenames, word_centroid_map, num_clusters, word_idf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(prob_wordvecs,open(\"../japanese-dataset/livedoor-news-corpus/model/prob_wordvecs.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gwbowv is a matrix which contains normalised document vectors.\n",
    "gwbowv = np.zeros( (train[\"news\"].size, num_clusters*(num_features)), dtype=\"float32\")\n",
    "\n",
    "counter = 0\n",
    "\n",
    "min_no = 0\n",
    "max_no = 0\n",
    "for review in train[\"news\"]:\n",
    "    # Get the wordlist in each news article.\n",
    "    result = tokenizer.parse(review).replace(\"\\u3000\",\"\").replace(\"\\n\",\"\")\n",
    "    result = re.sub(r'[0123456789０１２３４５６７８９！＠＃＄％＾＆\\-|\\\\＊\\“（）＿■×※⇒—●(：〜＋=)／*&^%$#@!~`){}…\\[\\]\\\"\\'\\”:;<>?＜＞？、。・,./『』【】「」→←○]+', \"\", result)\n",
    "    h = result.split(\" \")\n",
    "    h = filter((\"\").__ne__, h)\n",
    "    words = h\n",
    "    gwbowv[counter] = create_cluster_vector_and_gwbowv(prob_wordvecs, words, word_centroid_map, word_centroid_prob_map, num_features, word_idf_dict, featurenames, num_clusters, train=True)\n",
    "    counter+=1\n",
    "    if counter % 1000 == 0:\n",
    "        print (\"Train News Covered : \",counter)\n",
    "\n",
    "gwbowv_name = \"SDV_\" + str(num_clusters) + \"cluster_\" + str(num_features) + \"feature_matrix_gmm_sparse.npy\"\n",
    "\n",
    "gwbowv_test = np.zeros( (test[\"news\"].size, num_clusters*(num_features)), dtype=\"float32\")\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for review in test[\"news\"]:\n",
    "    # Get the wordlist in each news article.\n",
    "    result = tokenizer.parse(review).replace(\"\\u3000\",\"\").replace(\"\\n\",\"\")\n",
    "    result = re.sub(r'[0123456789０１２３４５６７８９！＠＃＄％＾＆\\-|\\\\＊\\“（）＿■×※⇒—●(：〜＋=)／*&^%$#@!~`){}…\\[\\]\\\"\\'\\”:;<>?＜＞？、。・,./『』【】「」→←○]+', \"\", result)\n",
    "    h = result.split(\" \")\n",
    "    h = filter((\"\").__ne__, h)\n",
    "    words = h\n",
    "    gwbowv_test[counter] = create_cluster_vector_and_gwbowv(prob_wordvecs, words, word_centroid_map, word_centroid_prob_map, num_features, word_idf_dict, featurenames, num_clusters)\n",
    "    counter+=1\n",
    "    if counter % 1000 == 0:\n",
    "        print (\"Test News Covered : \",counter)\n",
    "\n",
    "test_gwbowv_name = \"TEST_SDV_\" + str(num_clusters) + \"cluster_\" + str(num_features) + \"feature_matrix_gmm_sparse.npy\"\n",
    "\n",
    "print (\"Making sparse...\")\n",
    "# Set the threshold percentage for making it sparse. \n",
    "percentage = 0.04\n",
    "min_no = min_no*1.0/len(train[\"news\"])\n",
    "max_no = max_no*1.0/len(train[\"news\"])\n",
    "print (\"Average min: \", min_no)\n",
    "print (\"Average max: \", max_no)\n",
    "thres = (abs(max_no) + abs(min_no))/2\n",
    "thres = thres*percentage\n",
    "\n",
    "# Make values of matrices which are less than threshold to zero.\n",
    "temp = abs(gwbowv) < thres\n",
    "gwbowv[temp] = 0\n",
    "\n",
    "temp = abs(gwbowv_test) < thres\n",
    "gwbowv_test[temp] = 0\n",
    "\n",
    "#saving gwbowv train and test matrices\n",
    "np.save(\"../japanese-dataset/livedoor-news-corpus/model/\"+gwbowv_name, gwbowv)\n",
    "np.save(\"../japanese-dataset/livedoor-news-corpus/model/\"+test_gwbowv_name, gwbowv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all version\n",
    "counter = 0\n",
    "all_gwbowv_name = \"ALL_SDV_\" + str(num_clusters) + \"cluster_\" + str(num_features) + \"feature_matrix_gmm_sparse.npy\"\n",
    "gwbowv_all = np.zeros( (all[\"news\"].size, num_clusters*(num_features)), dtype=\"float32\")\n",
    "\n",
    "for review in all[\"news\"]:\n",
    "    # Get the wordlist in each news article.\n",
    "    result = tokenizer.parse(review).replace(\"\\u3000\",\"\").replace(\"\\n\",\"\")\n",
    "    result = re.sub(r'[0123456789０１２３４５６７８９！＠＃＄％＾＆\\-|\\\\＊\\“（）＿■×※⇒—●(：〜＋=)／*&^%$#@!~`){}…\\[\\]\\\"\\'\\”:;<>?＜＞？、。・,./『』【】「」→←○]+', \"\", result)\n",
    "    h = result.split(\" \")\n",
    "    h = filter((\"\").__ne__, h)\n",
    "    words = list(h)\n",
    "    gwbowv_all[counter] = create_cluster_vector_and_gwbowv(prob_wordvecs, words, word_centroid_map, word_centroid_prob_map, num_features, word_idf_dict, featurenames, num_clusters)\n",
    "    counter+=1\n",
    "    if counter % 1000 == 0:\n",
    "        print (\"All News Covered : \",counter)\n",
    "        \n",
    "print (\"Making sparse...\")\n",
    "# Set the threshold percentage for making it sparse. \n",
    "percentage = 0.04\n",
    "print (\"Average min: \", min_no)\n",
    "print (\"Average max: \", max_no)\n",
    "thres = (abs(max_no) + abs(min_no))/2\n",
    "thres = thres*percentage\n",
    "\n",
    "temp = abs(gwbowv_all) < thres\n",
    "gwbowv_all[temp] = 0\n",
    "\n",
    "np.save(\"../japanese-dataset/livedoor-news-corpus/model/\"+all_gwbowv_name,gwbowv_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "prob_wordvecs = pickle.load(open(\"../japanese-dataset/livedoor-news-corpus/model/prob_wordvecs.pkl\",\"rb\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prob_wordvecs[\"独身\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot modified word vector representation\n",
    "skip=0\n",
    "limit=241 \n",
    "\n",
    "vocab = list(prob_wordvecs.keys())\n",
    "tsne_target = []\n",
    "for i in range(limit):\n",
    "    tsne_target.append(prob_wordvecs[vocab[i]])\n",
    "X = np.vstack(tsne_target)\n",
    " \n",
    "tsne_model_scdv = TSNE(n_components=2, random_state=0,verbose=2)\n",
    "np.set_printoptions(suppress=True)\n",
    "tsne_model_scdv.fit_transform(X)\n",
    "pickle.dump(tsne_model_scdv,open(\"../japanese-dataset/livedoor-news-corpus/model/tsne_scdv.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_clusters = 60\n",
    "num_clusters = 1\n",
    "num_features = 200\n",
    "test_gwbowv_name = \"TEST_SDV_\" + str(num_clusters) + \"cluster_\" + str(num_features) + \"feature_matrix_gmm_sparse.npy\"\n",
    "gwbowv_name = \"SDV_\" + str(num_clusters) + \"cluster_\" + str(num_features) + \"feature_matrix_gmm_sparse.npy\"\n",
    "gwbowv = np.load(\"../japanese-dataset/livedoor-news-corpus/model/\"+gwbowv_name)\n",
    "gwbowv_test = np.load(\"../japanese-dataset/livedoor-news-corpus/model/\"+test_gwbowv_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test lgb\n",
    "from sklearn.metrics import classification_report\n",
    "import lightgbm as lgb\n",
    "\n",
    "strt = time.time()\n",
    "clf = lgb.LGBMClassifier(objective=\"multiclass\",random_state=40)\n",
    "clf.fit(gwbowv, train[\"class\"])\n",
    "Y_true, Y_pred  = test[\"class\"], clf.predict(gwbowv_test)\n",
    "print (\"Report\")\n",
    "print (classification_report(Y_true, Y_pred, digits=6))\n",
    "print (\"Accuracy: \",clf.score(gwbowv_test,test[\"class\"]))\n",
    "print (\"Time taken:\", time.time() - strt, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#document vector visualization\n",
    "## plain word2vec　or fasttext\n",
    "def plain_word2vec_document_vector(sentence,word2vec_model,num_features):\n",
    "    bag_of_centroids = np.zeros(num_features, dtype=\"float32\")\n",
    "\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            temp = word2vec_model[word]\n",
    "        except:\n",
    "            continue\n",
    "        bag_of_centroids += temp\n",
    "\n",
    "    bag_of_centroids =  bag_of_centroids / len(sentence)\n",
    "        \n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plainDocVec_all = {}\n",
    "counter = 0\n",
    "num_features = 200\n",
    "\n",
    "for review in all[\"news\"]:\n",
    "    # Get the wordlist in each news article.\n",
    "    result = tokenizer.parse(review).replace(\"\\u3000\",\"\").replace(\"\\n\",\"\")\n",
    "    result = re.sub(r'[0123456789０１２３４５６７８９！＠＃＄％＾＆\\-|\\\\＊\\“（）＿■×※⇒—●(：〜＋=)／*&^%$#@!~`){}…\\[\\]\\\"\\'\\”:;<>?＜＞？、。・,./『』【】「」→←○]+', \"\", result)\n",
    "    h = result.split(\" \")\n",
    "    h = filter((\"\").__ne__, h)\n",
    "    words = list(h)\n",
    "    plainDocVec_all[counter] = plain_word2vec_document_vector(words,word2vec_model,num_features)\n",
    "    counter+=1\n",
    "    if counter % 1000 == 0:\n",
    "        print (\"All News Covered : \",counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plainDocVec_train = {}\n",
    "counter = 0\n",
    "for review in train[\"news\"]:\n",
    "    # Get the wordlist in each news article.\n",
    "    result = tokenizer.parse(review).replace(\"\\u3000\",\"\").replace(\"\\n\",\"\")\n",
    "    result = re.sub(r'[0123456789０１２３４５６７８９！＠＃＄％＾＆\\-|\\\\＊\\“（）＿■×※⇒—●(：〜＋=)／*&^%$#@!~`){}…\\[\\]\\\"\\'\\”:;<>?＜＞？、。・,./『』【】「」→←○]+', \"\", result)\n",
    "    h = result.split(\" \")\n",
    "    h = filter((\"\").__ne__, h)\n",
    "    words = list(h)\n",
    "    plainDocVec_train[counter] = plain_word2vec_document_vector(words,word2vec_model,num_features)\n",
    "    counter+=1\n",
    "    if counter % 1000 == 0:\n",
    "        print (\"Train News Covered : \",counter)\n",
    "        \n",
    "plainDocVec_test = {}\n",
    "counter = 0\n",
    "for review in test[\"news\"]:\n",
    "    # Get the wordlist in each news article.\n",
    "    result = tokenizer.parse(review).replace(\"\\u3000\",\"\").replace(\"\\n\",\"\")\n",
    "    result = re.sub(r'[0123456789０１２３４５６７８９！＠＃＄％＾＆\\-|\\\\＊\\“（）＿■×※⇒—●(：〜＋=)／*&^%$#@!~`){}…\\[\\]\\\"\\'\\”:;<>?＜＞？、。・,./『』【】「」→←○]+', \"\", result)\n",
    "    h = result.split(\" \")\n",
    "    h = filter((\"\").__ne__, h)\n",
    "    words = list(h)\n",
    "    plainDocVec_test[counter] = plain_word2vec_document_vector(words,word2vec_model,num_features)\n",
    "    counter+=1\n",
    "    if counter % 1000 == 0:\n",
    "        print (\"Test News Covered : \",counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(plainDocVec_all,open(\"../japanese-dataset/livedoor-news-corpus/model/plaindocvec_all.pkl\",\"wb\"))\n",
    "pickle.dump(plainDocVec_train,open(\"../japanese-dataset/livedoor-news-corpus/model/plaindocvec_train.pkl\",\"wb\"))\n",
    "pickle.dump(plainDocVec_test,open(\"../japanese-dataset/livedoor-news-corpus/model/plaindocvec_test.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualize all document vector\n",
    "emb_tuple = tuple([plainDocVec_all[v] for v in plainDocVec_all.keys()])\n",
    "X = np.vstack(emb_tuple)\n",
    " \n",
    "plain_w2v_doc= TSNE(n_components=2, random_state=0,verbose=2)\n",
    "np.set_printoptions(suppress=True)\n",
    "plain_w2v_doc.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alldoc_plainw2v_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "alldoc_plainw2v_tsne = pd.DataFrame(plain_w2v_doc.embedding_[:, 0],columns = [\"x\"])\n",
    "alldoc_plainw2v_tsne[\"y\"] = pd.DataFrame(plain_w2v_doc.embedding_[:, 1])\n",
    "alldoc_plainw2v_tsne[\"class\"] = list(all[\"class\"])\n",
    "\n",
    "sns.lmplot(data=alldoc_plainw2v_tsne,x=\"x\",y=\"y\",hue=\"class\",fit_reg=False,size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test lgb plain\n",
    "from sklearn.metrics import classification_report\n",
    "import lightgbm as lgb\n",
    "\n",
    "strt = time.time()\n",
    "clf = lgb.LGBMClassifier(objective=\"multiclass\",random_state=40)\n",
    "clf.fit(np.array(list(plainDocVec_train.values())), train[\"class\"])\n",
    "Y_true, Y_pred  = test[\"class\"], clf.predict(np.array(list(plainDocVec_test.values())))\n",
    "print (\"Report\")\n",
    "print (classification_report(Y_true, Y_pred, digits=6))\n",
    "print (\"Accuracy: \",clf.score(np.array(list(plainDocVec_test.values())),test[\"class\"]))\n",
    "print (\"Time taken:\", time.time() - strt, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualize all document vector SCDV\n",
    "X = np.vstack(gwbowv_all)\n",
    " \n",
    "scdv_doc= TSNE(n_components=2, random_state=0,verbose=2)\n",
    "np.set_printoptions(suppress=True)\n",
    "scdv_doc.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldoc_scdv_tsne = pd.DataFrame(scdv_doc.embedding_[:, 0],columns = [\"x\"])\n",
    "alldoc_scdv_tsne[\"y\"] = pd.DataFrame(scdv_doc.embedding_[:, 1])\n",
    "alldoc_scdv_tsne[\"class\"] = list(all[\"class\"])\n",
    "alldoc_scdv_tsne.to_csv(\"alldoc_scdv_tsne.csv\")\n",
    "sns.lmplot(data=alldoc_scdv_tsne,x=\"x\",y=\"y\",hue=\"class\",fit_reg=False,size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create txt file for fasttext\n",
    "def create_txt_fasttext(all):\n",
    "    sentence = all[\"news\"]\n",
    "    res = []\n",
    "    for doc in sentence:\n",
    "        result = tokenizer.parse(doc).replace(\"\\u3000\",\"\").replace(\"\\n\",\"\")\n",
    "        result = re.sub(r'[0123456789０１２３４５６７８９！＠＃＄％＾＆\\-|\\\\＊\\“（）＿■×※⇒—●(：〜＋=)／*&^%$#@!~`){}…\\[\\]\\\"\\'\\”:;<>?＜＞？、。・,./『』【】「」→←○]+', \"\", result)\n",
    "        h = result.split(\" \")\n",
    "        h = list(filter((\"\").__ne__, h))\n",
    "        res.append(\" \".join(h))\n",
    "    return \" \".join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = create_txt_fasttext(all)\n",
    "f = codecs.open(\"../japanese-dataset/livedoor-news-corpus/for-fasttext/corpus.txt\", 'w', 'utf-8') #\n",
    "f.write(all_words) \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "fasttext_model = fasttext.train_unsupervised('../japanese-dataset/livedoor-news-corpus/for-fasttext/corpus.txt', model='skipgram')\n",
    "fasttext_model.save_model(\"model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttextDocVec_train = {}\n",
    "counter = 0\n",
    "num_features_fasttext = 100\n",
    "for review in train[\"news\"]:\n",
    "    # Get the wordlist in each news article.\n",
    "    result = tokenizer.parse(review).replace(\"\\u3000\",\"\").replace(\"\\n\",\"\")\n",
    "    result = re.sub(r'[0123456789０１２３４５６７８９！＠＃＄％＾＆\\-|\\\\＊\\“（）＿■×※⇒—●(：〜＋=)／*&^%$#@!~`){}…\\[\\]\\\"\\'\\”:;<>?＜＞？、。・,./『』【】「」→←○]+', \"\", result)\n",
    "    h = result.split(\" \")\n",
    "    h = filter((\"\").__ne__, h)\n",
    "    words = list(h)\n",
    "    fasttextDocVec_train[counter] = plain_word2vec_document_vector(words,fasttext_model,num_features_fasttext)\n",
    "    counter+=1\n",
    "    if counter % 1000 == 0:\n",
    "        print (\"Train News Covered : \",counter)\n",
    "        \n",
    "fasttextDocVec_test = {}\n",
    "counter = 0\n",
    "for review in test[\"news\"]:\n",
    "    # Get the wordlist in each news article.\n",
    "    result = tokenizer.parse(review).replace(\"\\u3000\",\"\").replace(\"\\n\",\"\")\n",
    "    result = re.sub(r'[0123456789０１２３４５６７８９！＠＃＄％＾＆\\-|\\\\＊\\“（）＿■×※⇒—●(：〜＋=)／*&^%$#@!~`){}…\\[\\]\\\"\\'\\”:;<>?＜＞？、。・,./『』【】「」→←○]+', \"\", result)\n",
    "    h = result.split(\" \")\n",
    "    h = filter((\"\").__ne__, h)\n",
    "    words = list(h)\n",
    "    fasttextDocVec_test[counter] = plain_word2vec_document_vector(words,fasttext_model,num_features_fasttext)\n",
    "    counter+=1\n",
    "    if counter % 1000 == 0:\n",
    "        print (\"Test News Covered : \",counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fasttext\n",
    "strt = time.time()\n",
    "clf = lgb.LGBMClassifier(objective=\"multiclass\",random_state=40)\n",
    "clf.fit(np.array(list(fasttextDocVec_train.values())), train[\"class\"])\n",
    "Y_true, Y_pred  = test[\"class\"], clf.predict(np.array(list(fasttextDocVec_test.values())))\n",
    "print (\"Report\")\n",
    "print (classification_report(Y_true, Y_pred, digits=6))\n",
    "print (\"Accuracy: \",clf.score(np.array(list(fasttextDocVec_test.values())),test[\"class\"]))\n",
    "print (\"Time taken:\", time.time() - strt, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "from gensim.models.doc2vec import LabeledSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_sentence(doc, name):\n",
    "    result = tokenizer.parse(doc).replace(\"\\u3000\",\"\").replace(\"\\n\",\"\")\n",
    "    result = re.sub(r'[0123456789０１２３４５６７８９！＠＃＄％＾＆\\-|\\\\＊\\“（）＿■×※⇒—●(：〜＋=)／*&^%$#@!~`){}…\\[\\]\\\"\\'\\”:;<>?＜＞？、。・,./『』【】「」→←○]+', \"\", result)\n",
    "    h = result.split(\" \")\n",
    "    h = list(filter((\"\").__ne__, h))\n",
    "    return LabeledSentence(words=h, tags=[name])\n",
    "\n",
    "def corpus_to_sentences(df):\n",
    "    docs   = df[\"news\"]\n",
    "    res = []\n",
    "    for i in range(len(docs)):\n",
    "        t = doc_to_sentence(docs[i],i)\n",
    "        res.append(t)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = corpus_to_sentences(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2_model = models.Doc2Vec(dm=0, size=300, window=15, alpha=.025,\n",
    "        min_alpha=.025, min_count=1, sample=1e-6)\n",
    "doc2_model.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(10)):\n",
    "    doc2_model.train(sentences,total_examples=len(all),epochs=word2vec_model.iter)\n",
    "    doc2_model.alpha -= 0.002  # decrease the learning rate`\n",
    "    doc2_model.min_alpha = doc2_model.alpha  # fix the learning rate, no decay\n",
    "doc2_model.save(\"../japanese-dataset/livedoor-news-corpus/model/doc2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2_train = []\n",
    "for i in train.index:\n",
    "    doc2_train.append(doc2_model.docvecs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2_test = []\n",
    "for i in test.index:\n",
    "    doc2_test.append(doc2_model.docvecs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test lgb doc2vec\n",
    "strt = time.time()\n",
    "clf = lgb.LGBMClassifier(objective=\"multiclass\",random_state=40)\n",
    "clf.fit(np.array(doc2_train), train[\"class\"])\n",
    "Y_true, Y_pred  = test[\"class\"], clf.predict(np.array(doc2_test))\n",
    "print (\"Report\")\n",
    "print (classification_report(Y_true, Y_pred, digits=6))\n",
    "print (\"Accuracy: \",clf.score(np.array(doc2_test),test[\"class\"]))\n",
    "print (\"Time taken:\", time.time() - strt, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
