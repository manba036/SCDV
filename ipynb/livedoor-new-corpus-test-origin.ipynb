{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import codecs\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirlist = [\"dokujo-tsushin\",\"it-life-hack\",\"kaden-channel\",\"livedoor-homme\",\n",
    "           \"movie-enter\",\"peachy\",\"smax\",\"sports-watch\",\"topic-news\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"class\",\"news\"])\n",
    "for i in tqdm(dirlist):\n",
    "    path = \"../japanese-dataset/livedoor-news-corpus/\"+i+\"/*.txt\"\n",
    "    files = glob.glob(path)\n",
    "    files.pop()\n",
    "    for j in tqdm(files):\n",
    "        f = codecs.open(j, 'r', 'utf-8')\n",
    "        data = f.read() \n",
    "        f.close()\n",
    "        t = pd.Series([i,\"\".join(data.split(\"\\n\")[3:])],index = df.columns)\n",
    "        df  = df.append(t,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) SCDV based word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import pickle\n",
    "import MeCab\n",
    "import time\n",
    "from sklearn.preprocessing import normalize\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "tokenizer =  MeCab.Tagger(\"-Owakati\")  \n",
    "sentences = []\n",
    "print (\"Parsing sentences from training set...\")\n",
    "\n",
    "# Loop over each news article.\n",
    "for review in tqdm(df[\"news\"]):\n",
    "    try:\n",
    "        # Split a review into parsed sentences.\n",
    "        result = tokenizer.parse(review).replace(\"\\u3000\",\"\").replace(\"\\n\",\"\")\n",
    "        result = re.sub(r'[0123456789０１２３４５６７８９！＠＃＄％＾＆\\-|\\\\＊\\“（）＿■×※⇒—●(：〜＋=)／*&^%$#@!~`){}…\\[\\]\\\"\\'\\”:;<>?＜＞？、。・,./『』【】「」→←○]+', \"\", result)\n",
    "        h = result.split(\" \")\n",
    "        h = list(filter((\"\").__ne__, h))\n",
    "        sentences.append(h)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "num_features = 200     # Word vector dimensionality\n",
    "min_word_count = 20   # Minimum word count\n",
    "num_workers = 40       # Number of threads to run in parallel\n",
    "context = 10          # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "print (\"Training Word2Vec model...\")\n",
    "# Train Word2Vec model.\n",
    "model = Word2Vec(sentences, workers=num_workers, hs = 0, sg = 1, negative = 10, iter = 25,\\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling, seed=1)\n",
    "\n",
    "model_name = str(num_features) + \"features_\" + str(min_word_count) + \"minwords_\" + str(context) + \"context_len2alldata\"\n",
    "model.init_sims(replace=True)\n",
    "# Save Word2Vec model.\n",
    "print (\"Saving Word2Vec model...\")\n",
    "model.save(\"../japanese-dataset/livedoor-news-corpus/model/\"+model_name)\n",
    "endmodeltime = time.time()\n",
    "\n",
    "print (\"time : \", endmodeltime-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.wv.save_word2vec_format(\"/Users/01018534/Downloads/testw2v.txt\",binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plain word2vec t-SNE Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model=pickle.load(open(\"../japanese-dataset/livedoor-news-corpus/model/\"+model_name,\"rb\"))\n",
    " \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "skip=0\n",
    "limit=241 \n",
    "\n",
    "vocab = word2vec_model.wv.vocab\n",
    "emb_tuple = tuple([word2vec_model[v] for v in vocab])\n",
    "X = np.vstack(emb_tuple)\n",
    " \n",
    "tsne_model = TSNE(n_components=2, random_state=0,verbose=2)\n",
    "np.set_printoptions(suppress=True)\n",
    "tsne_model.fit_transform(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tsne_model,open(\"../japanese-dataset/livedoor-news-corpus/model/tsne_plain.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create gwbowv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer,HashingVectorizer\n",
    "import pickle\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drange(start, stop, step):\n",
    "    r = start\n",
    "    while r < stop:\n",
    "        yield r\n",
    "        r += step\n",
    "\n",
    "def cluster_GMM(num_clusters, word_vectors):\n",
    "    # Initalize a GMM object and use it for clustering.\n",
    "    clf =  GaussianMixture(n_components=num_clusters,\n",
    "                    covariance_type=\"tied\", init_params='kmeans', max_iter=50)\n",
    "    # Get cluster assignments.\n",
    "    clf.fit(word_vectors)\n",
    "    idx = clf.predict(word_vectors)\n",
    "    print (\"Clustering Done...\", time.time()-start, \"seconds\")\n",
    "    # Get probabilities of cluster assignments.\n",
    "    idx_proba = clf.predict_proba(word_vectors)\n",
    "    # Dump cluster assignments and probability of cluster assignments. \n",
    "    pickle.dump(idx, open('../japanese-dataset/livedoor-news-corpus/model/gmm_latestclusmodel_len2alldata.pkl',\"wb\"))\n",
    "    print (\"Cluster Assignments Saved...\")\n",
    "\n",
    "    pickle.dump(idx_proba,open( '../japanese-dataset/livedoor-news-corpus/model/gmm_prob_latestclusmodel_len2alldata.pkl',\"wb\"))\n",
    "    print (\"Probabilities of Cluster Assignments Saved...\")\n",
    "    return (idx, idx_proba)\n",
    "\n",
    "def read_GMM(idx_name, idx_proba_name):\n",
    "    # Loads cluster assignments and probability of cluster assignments. \n",
    "    idx = pickle.load(open('../japanese-dataset/livedoor-news-corpus/model/gmm_latestclusmodel_len2alldata.pkl',\"rb\"))\n",
    "    idx_proba = pickle.load(open( '../japanese-dataset/livedoor-news-corpus/model/gmm_prob_latestclusmodel_len2alldata.pkl',\"rb\"))\n",
    "    print (\"Cluster Model Loaded...\")\n",
    "    return (idx, idx_proba)\n",
    "\n",
    "def get_probability_word_vectors(featurenames, model,word_centroid_map, num_clusters, word_idf_dict):\n",
    "    # This function computes probability word-cluster vectors\n",
    "    prob_wordvecs = {}\n",
    "    for word in word_centroid_map:\n",
    "        prob_wordvecs[word] = np.zeros( num_clusters * num_features, dtype=\"float32\" )\n",
    "        for index in range(0, num_clusters):\n",
    "            try:\n",
    "                prob_wordvecs[word][index*num_features:(index+1)*num_features] = model[word] * word_centroid_prob_map[word][index] * word_idf_dict[word]\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    # prob_wordvecs_idf_len2alldata = {}\n",
    "    # i = 0\n",
    "    # for word in featurenames:\n",
    "    #     i += 1\n",
    "    #     if word in word_centroid_map:    \n",
    "    #         prob_wordvecs_idf_len2alldata[word] = {}\n",
    "    #         for index in range(0, num_clusters):\n",
    "    #                 prob_wordvecs_idf_len2alldata[word][index] = model[word] * word_centroid_prob_map[word][index] * word_idf_dict[word] \n",
    "\n",
    "    \n",
    "\n",
    "    # for word in prob_wordvecs_idf_len2alldata.keys():\n",
    "    #     prob_wordvecs[word] = prob_wordvecs_idf_len2alldata[word][0]\n",
    "    #     for index in prob_wordvecs_idf_len2alldata[word].keys():\n",
    "    #         if index==0:\n",
    "    #             continue\n",
    "    #         prob_wordvecs[word] = np.concatenate((prob_wordvecs[word], prob_wordvecs_idf_len2alldata[word][index]))\n",
    "    return prob_wordvecs\n",
    "\n",
    "def create_cluster_vector_and_gwbowv(prob_wordvecs, wordlist, word_centroid_map, word_centroid_prob_map, dimension, word_idf_dict, featurenames, num_centroids, train=False):\n",
    "    # This function computes SDV feature vectors.\n",
    "    bag_of_centroids = np.zeros( num_centroids * dimension, dtype=\"float32\" )\n",
    "    global min_no\n",
    "    global max_no\n",
    "\n",
    "    for word in wordlist:\n",
    "        try:\n",
    "            temp = word_centroid_map[word]\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        bag_of_centroids += prob_wordvecs[word]\n",
    "\n",
    "    norm = np.sqrt(np.einsum('...i,...i', bag_of_centroids, bag_of_centroids))\n",
    "    if(norm!=0):\n",
    "        bag_of_centroids /= norm\n",
    "\n",
    "    # To make feature vector sparse, make note of minimum and maximum values.\n",
    "    if train:\n",
    "        min_no += min(bag_of_centroids)\n",
    "        max_no += max(bag_of_centroids)\n",
    "\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 200     # Word vector dimensionality\n",
    "min_word_count = 20   # Minimum word count\n",
    "num_workers = 40       # Number of threads to run in parallel\n",
    "context = 10          # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "model_name = str(num_features) + \"features_\" + str(min_word_count) + \"minwords_\" + str(context) + \"context_len2alldata\"\n",
    "# Load the trained Word2Vec model.\n",
    "model = Word2Vec.load(\"../japanese-dataset/livedoor-news-corpus/model/\"+model_name)\n",
    "# Get wordvectors for all words in vocabulary.\n",
    "word_vectors = model.wv.syn0\n",
    "\n",
    "# Load train data.\n",
    "train,test = train_test_split(df,test_size=0.3,random_state=40)\n",
    "all = df\n",
    "\n",
    "# Set number of clusters.\n",
    "#num_clusters = 40\n",
    "num_clusters = 10\n",
    "# Uncomment below line for creating new clusters.\n",
    "idx, idx_proba = cluster_GMM(num_clusters, word_vectors)\n",
    "\n",
    "# Uncomment below lines for loading saved cluster assignments and probabaility of cluster assignments.\n",
    "# idx_name = \"gmm_latestclusmodel_len2alldata.pkl\"\n",
    "# idx_proba_name = \"gmm_prob_latestclusmodel_len2alldata.pkl\"\n",
    "# idx, idx_proba = read_GMM(idx_name, idx_proba_name)\n",
    "\n",
    "# Create a Word / Index dictionary, mapping each vocabulary word to\n",
    "# a cluster number\n",
    "word_centroid_map = dict(zip( model.wv.index2word, idx ))\n",
    "# Create a Word / Probability of cluster assignment dictionary, mapping each vocabulary word to\n",
    "# list of probabilities of cluster assignments.\n",
    "word_centroid_prob_map = dict(zip( model.wv.index2word, idx_proba ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## color scatter map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot') \n",
    "plainw2v = pd.DataFrame(tsne_model.embedding_[skip:limit, 0],columns = [\"x\"])\n",
    "plainw2v[\"y\"] = pd.DataFrame(tsne_model.embedding_[skip:limit, 1])\n",
    "plainw2v[\"word\"] = list(vocab)[skip:limit]\n",
    "plainw2v[\"cluster\"] = idx[skip:limit]\n",
    "plainw2v.plot.scatter(x=\"x\",y=\"y\",c=\"cluster\",cmap=\"viridis\",figsize=(8, 6),s=30)\n",
    "\n",
    "# import plotly\n",
    "# import plotly.plotly as py\n",
    "# import plotly.graph_objs as go\n",
    "# plotly.offline.init_notebook_mode(connected=False)\n",
    "# # Create a trace\n",
    "# trace = go.Scatter(\n",
    "#     x =model.embedding_[skip:limit, 0],\n",
    "#     y = model.embedding_[skip:limit, 1],\n",
    "        \n",
    "#     text=list(vocab.keys()),\n",
    "#     mode = 'markers+text'\n",
    "# )\n",
    "\n",
    "# data = [trace]\n",
    "\n",
    "# # Plot and embed in ipython notebook!\n",
    "# plotly.offline.iplot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing tf-idf values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = []\n",
    "for review in all[\"news\"]:\n",
    "    result = tokenizer.parse(review).replace(\"\\u3000\",\"\").replace(\"\\n\",\"\")\n",
    "    result = re.sub(r'[0123456789０１２３４５６７８９！＠＃＄％＾＆\\-|\\\\＊\\“（）＿■×※⇒—●(：〜＋=)／*&^%$#@!~`){}…\\[\\]\\\"\\'\\”:;<>?＜＞？、。・,./『』【】「」→←○]+', \"\", result)\n",
    "    h = result.split(\" \")\n",
    "    h = filter((\"\").__ne__, h)\n",
    "    traindata.append(\" \".join(h))\n",
    "\n",
    "tfv = TfidfVectorizer(dtype=np.float32)\n",
    "tfidfmatrix_traindata = tfv.fit_transform(traindata)\n",
    "featurenames = tfv.get_feature_names()\n",
    "idf = tfv._tfidf.idf_\n",
    "\n",
    "# Creating a dictionary with word mapped to its idf value \n",
    "print (\"Creating word-idf dictionary for Training set...\")\n",
    "\n",
    "word_idf_dict = {}\n",
    "for pair in zip(featurenames, idf):\n",
    "    word_idf_dict[pair[0]] = pair[1]\n",
    "    \n",
    "# Pre-computing probability word-cluster vectors.\n",
    "prob_wordvecs = get_probability_word_vectors(featurenames, model,word_centroid_map, num_clusters, word_idf_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for gensim keyedvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "file = codecs.open('../japanese-dataset/livedoor-news-corpus/model/temp2.txt', 'w', 'utf-8')\n",
    "string_list = [str(len(prob_wordvecs)),str(len(list(prob_wordvecs.values())[0]))]\n",
    "string_list.append(\"\\n\")\n",
    "file.writelines(\" \".join(string_list))\n",
    "for key,value in tqdm(prob_wordvecs.items()):\n",
    "    string_list = []\n",
    "    string_list.append(key)\n",
    "    for i in value:\n",
    "        string_list.append(str(i))\n",
    "    string_list.append(\"\\n\")\n",
    "    file.writelines(\" \".join(string_list))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_weighted = KeyedVectors.load_word2vec_format(\"../japanese-dataset/livedoor-news-corpus/model/temp2.txt\",binary=False)\n",
    "word2vec_weighted.save(\"../japanese-dataset/livedoor-news-corpus/model/vector-response-test/word2vec_weighted.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(prob_wordvecs,open(\"../japanese-dataset/livedoor-news-corpus/model/prob_wordvecs.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gwbowv is a matrix which contains normalised document vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwbowv = np.zeros( (train[\"news\"].size, num_clusters*(num_features)), dtype=\"float32\")\n",
    "\n",
    "counter = 0\n",
    "\n",
    "min_no = 0\n",
    "max_no = 0\n",
    "for review in train[\"news\"]:\n",
    "    # Get the wordlist in each news article.\n",
    "    result = tokenizer.parse(review).replace(\"\\u3000\",\"\").replace(\"\\n\",\"\")\n",
    "    result = re.sub(r'[0123456789０１２３４５６７８９！＠＃＄％＾＆\\-|\\\\＊\\“（）＿■×※⇒—●(：〜＋=)／*&^%$#@!~`){}…\\[\\]\\\"\\'\\”:;<>?＜＞？、。・,./『』【】「」→←○]+', \"\", result)\n",
    "    h = result.split(\" \")\n",
    "    h = filter((\"\").__ne__, h)\n",
    "    words = h\n",
    "    gwbowv[counter] = create_cluster_vector_and_gwbowv(prob_wordvecs, words, word_centroid_map, word_centroid_prob_map, num_features, word_idf_dict, featurenames, num_clusters, train=True)\n",
    "    counter+=1\n",
    "    if counter % 1000 == 0:\n",
    "        print (\"Train News Covered : \",counter)\n",
    "\n",
    "gwbowv_name = \"SDV_\" + str(num_clusters) + \"cluster_\" + str(num_features) + \"feature_matrix_gmm_sparse.npy\"\n",
    "\n",
    "gwbowv_test = np.zeros( (test[\"news\"].size, num_clusters*(num_features)), dtype=\"float32\")\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for review in test[\"news\"]:\n",
    "    # Get the wordlist in each news article.\n",
    "    result = tokenizer.parse(review).replace(\"\\u3000\",\"\").replace(\"\\n\",\"\")\n",
    "    result = re.sub(r'[0123456789０１２３４５６７８９！＠＃＄％＾＆\\-|\\\\＊\\“（）＿■×※⇒—●(：〜＋=)／*&^%$#@!~`){}…\\[\\]\\\"\\'\\”:;<>?＜＞？、。・,./『』【】「」→←○]+', \"\", result)\n",
    "    h = result.split(\" \")\n",
    "    h = filter((\"\").__ne__, h)\n",
    "    words = h\n",
    "    gwbowv_test[counter] = create_cluster_vector_and_gwbowv(prob_wordvecs, words, word_centroid_map, word_centroid_prob_map, num_features, word_idf_dict, featurenames, num_clusters)\n",
    "    counter+=1\n",
    "    if counter % 1000 == 0:\n",
    "        print (\"Test News Covered : \",counter)\n",
    "\n",
    "test_gwbowv_name = \"TEST_SDV_\" + str(num_clusters) + \"cluster_\" + str(num_features) + \"feature_matrix_gmm_sparse.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gwbowv[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Making sparse...\")\n",
    "# Set the threshold percentage for making it sparse. \n",
    "percentage = 0.04\n",
    "min_no = min_no*1.0/len(train[\"news\"])\n",
    "max_no = max_no*1.0/len(train[\"news\"])\n",
    "print (\"Average min: \", min_no)\n",
    "print (\"Average max: \", max_no)\n",
    "thres = (abs(max_no) + abs(min_no))/2\n",
    "thres = thres*percentage\n",
    "\n",
    "# Make values of matrices which are less than threshold to zero.\n",
    "temp = abs(gwbowv) < thres\n",
    "gwbowv[temp] = 0\n",
    "\n",
    "temp = abs(gwbowv_test) < thres\n",
    "gwbowv_test[temp] = 0\n",
    "\n",
    "#saving gwbowv train and test matrices\n",
    "np.save(\"../japanese-dataset/livedoor-news-corpus/model/\"+gwbowv_name, gwbowv)\n",
    "np.save(\"../japanese-dataset/livedoor-news-corpus/model/\"+test_gwbowv_name, gwbowv_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot modified word vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "skip=0\n",
    "limit=241 \n",
    "\n",
    "vocab = list(prob_wordvecs.keys())\n",
    "tsne_target = []\n",
    "for i in range(limit):\n",
    "    tsne_target.append(prob_wordvecs[vocab[i]])\n",
    "X = np.vstack(tsne_target)\n",
    " \n",
    "tsne_model_scdv = TSNE(n_components=2, random_state=0,verbose=2)\n",
    "np.set_printoptions(suppress=True)\n",
    "tsne_model_scdv.fit_transform(X)\n",
    "pickle.dump(tsne_model_scdv,open(\"../japanese-dataset/livedoor-news-corpus/model/tsne_scdv.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scdv_tsne = pd.DataFrame(tsne_model_scdv.embedding_[skip:limit, 0],columns = [\"x\"])\n",
    "scdv_tsne[\"y\"] = pd.DataFrame(tsne_model_scdv.embedding_[skip:limit, 1])\n",
    "scdv_tsne[\"word\"] = list(vocab)[skip:limit]\n",
    "scdv_tsne[\"cluster\"] = idx[skip:limit]\n",
    "scdv_tsne.plot.scatter(x=\"x\",y=\"y\",c=\"cluster\",cmap=\"viridis\",figsize=(8, 6),s=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly\n",
    "# import plotly.plotly as py\n",
    "# import plotly.graph_objs as go\n",
    "# plotly.offline.init_notebook_mode(connected=False)\n",
    "# # Create a trace\n",
    "# trace = go.Scatter(\n",
    "#     x =tsne_model_scdv.embedding_[skip:limit, 0],\n",
    "#     y = tsne_model_scdv.embedding_[skip:limit, 1],\n",
    "#     text=list(vocab)[skip:limit],\n",
    "#     mode = 'markers+text'\n",
    "# )\n",
    "\n",
    "# data = [trace]\n",
    "\n",
    "# # Plot and embed in ipython notebook!\n",
    "# plotly.offline.iplot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test lgb SCDV based word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import lightgbm as lgb\n",
    "\n",
    "start = time.time()\n",
    "clf = lgb.LGBMClassifier(objective=\"multiclass\")\n",
    "clf.fit(gwbowv, train[\"class\"])\n",
    "Y_true, Y_pred  = test[\"class\"], clf.predict(gwbowv_test)\n",
    "print (\"Report\")\n",
    "print (classification_report(Y_true, Y_pred, digits=6))\n",
    "print (\"Accuracy: \",clf.score(gwbowv_test,test[\"class\"]))\n",
    "print (\"Time taken:\", time.time() - start, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) fasttext-average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for review in all[\"news\"]:\n",
    "    # Get the wordlist in each news article.\n",
    "    result = tokenizer.parse(review).replace(\"\\u3000\",\"\").replace(\"\\n\",\"\")\n",
    "    result = re.sub(r'[0123456789０１２３４５６７８９！＠＃＄％＾＆\\-|\\\\＊\\“（）＿■×※⇒—●(：〜＋=)／*&^%$#@!~`){}…\\[\\]\\\"\\'\\”:;<>?＜＞？、。・,./『』【】「」→←○]+', \"\", result)\n",
    "    h = result.split(\" \")\n",
    "    h = filter((\"\").__ne__, h)\n",
    "    words = list(h)\n",
    "    res.append(\" \".join(words))\n",
    "corpus = \" \".join(res)\n",
    "f = codecs.open('../japanese-dataset/livedoor-news-corpus/for-fasttext/corpus.txt', 'w', 'utf-8') # 書き込みモードで開く\n",
    "f.write(corpus) # 引数の文字列をファイルに書き込む\n",
    "f.close() # ファイルを閉じる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models.wrappers import FastText\n",
    "# fasttext_model = FastText.load_fasttext_format('../japanese-dataset/livedoor-news-corpus/for-fasttext/fasttext_model_200dim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "fasttext_model = fasttext.train_unsupervised('../japanese-dataset/livedoor-news-corpus/for-fasttext/corpus.txt',\n",
    "                            model='skipgram',\n",
    "                            dim=200)\n",
    "fasttext_model.save_model('../japanese-dataset/livedoor-news-corpus/for-fasttext/fasttext_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plain_docvec(num_features, wordlist,model,train=False):\n",
    "    # This function computes SDV feature vectors.\n",
    "    bag_of_centroids = np.zeros( num_features, dtype=\"float32\" )\n",
    "    global min_no\n",
    "    global max_no\n",
    "\n",
    "    for word in wordlist:\n",
    "        bag_of_centroids += model[word]\n",
    "\n",
    "    norm = np.sqrt(np.einsum('...i,...i', bag_of_centroids, bag_of_centroids))\n",
    "    if(norm!=0):\n",
    "        bag_of_centroids /= norm\n",
    "\n",
    "    # To make feature vector sparse, make note of minimum and maximum values.\n",
    "    if train:\n",
    "        min_no += min(bag_of_centroids)\n",
    "        max_no += max(bag_of_centroids)\n",
    "\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gwbowv is a matrix which contains normalised document vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 200\n",
    "plain_fasttext = np.zeros( (train[\"news\"].size, num_features), dtype=\"float32\")\n",
    "\n",
    "counter = 0\n",
    "\n",
    "min_no = 0\n",
    "max_no = 0\n",
    "for review in tqdm(train[\"news\"]):\n",
    "    # Get the wordlist in each news article.\n",
    "    result = tokenizer.parse(review).replace(\"\\u3000\",\"\").replace(\"\\n\",\"\")\n",
    "    result = re.sub(r'[0123456789０１２３４５６７８９！＠＃＄％＾＆\\-|\\\\＊\\“（）＿■×※⇒—●(：〜＋=)／*&^%$#@!~`){}…\\[\\]\\\"\\'\\”:;<>?＜＞？、。・,./『』【】「」→←○]+', \"\", result)\n",
    "    h = result.split(\" \")\n",
    "    h = filter((\"\").__ne__, h)\n",
    "    words = list(h)\n",
    "    plain_fasttext[counter] = plain_docvec(num_features, words, fasttext_model,train=True)\n",
    "    counter+=1\n",
    "\n",
    "plain_fasttext_test = np.zeros( (test[\"news\"].size, num_features), dtype=\"float32\")\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for review in tqdm(test[\"news\"]):\n",
    "    # Get the wordlist in each news article.\n",
    "    result = tokenizer.parse(review).replace(\"\\u3000\",\"\").replace(\"\\n\",\"\")\n",
    "    result = re.sub(r'[0123456789０１２３４５６７８９！＠＃＄％＾＆\\-|\\\\＊\\“（）＿■×※⇒—●(：〜＋=)／*&^%$#@!~`){}…\\[\\]\\\"\\'\\”:;<>?＜＞？、。・,./『』【】「」→←○]+', \"\", result)\n",
    "    h = result.split(\" \")\n",
    "    h = filter((\"\").__ne__, h)\n",
    "    words = list(h)\n",
    "    plain_fasttext_test[counter] = plain_docvec(num_features, words, fasttext_model,train=False)\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test lgb fasttext-average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import lightgbm as lgb\n",
    "\n",
    "start = time.time()\n",
    "clf = lgb.LGBMClassifier(objective=\"multiclass\")\n",
    "clf.fit(plain_fasttext, train[\"class\"])\n",
    "Y_true, Y_pred  = test[\"class\"], clf.predict(plain_fasttext_test)\n",
    "print (\"Report\")\n",
    "print (classification_report(Y_true, Y_pred, digits=6))\n",
    "print (\"Accuracy: \",clf.score(plain_fasttext_test,test[\"class\"]))\n",
    "print (\"Time taken:\", time.time() - start, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) SCDV based fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.wrappers.fasttext import FastText\n",
    "fasttext_model_200 = FastText.load_fasttext_format('../japanese-dataset/livedoor-news-corpus/for-fasttext/fasttext_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get wordvectors for all words in vocabulary.\n",
    "word_vectors = fasttext_model_200.wv.syn0\n",
    "\n",
    "# Set number of clusters.\n",
    "#num_clusters = 60\n",
    "num_clusters = 10\n",
    "# Uncomment below line for creating new clusters.\n",
    "idx, idx_proba = cluster_GMM(num_clusters, word_vectors)\n",
    "\n",
    "# Uncomment below lines for loading saved cluster assignments and probabaility of cluster assignments.\n",
    "# idx_name = \"gmm_latestclusmodel_len2alldata.pkl\"\n",
    "# idx_proba_name = \"gmm_prob_latestclusmodel_len2alldata.pkl\"\n",
    "# idx, idx_proba = read_GMM(idx_name, idx_proba_name)\n",
    "\n",
    "# Create a Word / Index dictionary, mapping each vocabulary word to\n",
    "# a cluster number\n",
    "word_centroid_map = dict(zip( fasttext_model_200.wv.index2word, idx ))\n",
    "# Create a Word / Probability of cluster assignment dictionary, mapping each vocabulary word to\n",
    "# list of probabilities of cluster assignments.\n",
    "word_centroid_prob_map = dict(zip( fasttext_model_200.wv.index2word, idx_proba ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-computing probability word-cluster vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_wordvecs = get_probability_word_vectors(featurenames, fasttext_model_200,word_centroid_map, num_clusters, word_idf_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for gensim keyedvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = codecs.open('../japanese-dataset/livedoor-news-corpus/model/temp3.txt', 'w', 'utf-8')\n",
    "string_list = [str(len(prob_wordvecs)),str(len(list(prob_wordvecs.values())[0]))]\n",
    "string_list.append(\"\\n\")\n",
    "file.writelines(\" \".join(string_list))\n",
    "for key,value in tqdm(prob_wordvecs.items()):\n",
    "    string_list = []\n",
    "    string_list.append(key)\n",
    "    for i in value:\n",
    "        string_list.append(str(i))\n",
    "    string_list.append(\"\\n\")\n",
    "    file.writelines(\" \".join(string_list))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_weighted = KeyedVectors.load_word2vec_format(\"../japanese-dataset/livedoor-news-corpus/model/temp3.txt\",binary=False)\n",
    "fasttext_weighted.save(\"../japanese-dataset/livedoor-news-corpus/model/vector-response-test/fasttext_weighted.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(prob_wordvecs,open(\"../japanese-dataset/livedoor-news-corpus/model/prob_wordvecs_fasttext.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gwbowv is a matrix which contains normalised document vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwbowv_fasttext = np.zeros( (train[\"news\"].size, num_clusters*(num_features)), dtype=\"float32\")\n",
    "\n",
    "counter = 0\n",
    "\n",
    "min_no = 0\n",
    "max_no = 0\n",
    "for review in train[\"news\"]:\n",
    "    # Get the wordlist in each news article.\n",
    "    result = tokenizer.parse(review).replace(\"\\u3000\",\"\").replace(\"\\n\",\"\")\n",
    "    result = re.sub(r'[0123456789０１２３４５６７８９！＠＃＄％＾＆\\-|\\\\＊\\“（）＿■×※⇒—●(：〜＋=)／*&^%$#@!~`){}…\\[\\]\\\"\\'\\”:;<>?＜＞？、。・,./『』【】「」→←○]+', \"\", result)\n",
    "    h = result.split(\" \")\n",
    "    h = filter((\"\").__ne__, h)\n",
    "    words = list(h)\n",
    "    gwbowv_fasttext[counter] = create_cluster_vector_and_gwbowv(prob_wordvecs, words, word_centroid_map, word_centroid_prob_map, num_features, word_idf_dict, featurenames, num_clusters, train=True)\n",
    "    counter+=1\n",
    "    if counter % 1000 == 0:\n",
    "        print (\"Train News Covered : \",counter)\n",
    "\n",
    "gwbowv_name = \"SDV_fasttext_\" + str(num_clusters) + \"cluster_\" + str(num_features) + \"feature_matrix_gmm_sparse.npy\"\n",
    "\n",
    "gwbowv_fasttext_test = np.zeros( (test[\"news\"].size, num_clusters*(num_features)), dtype=\"float32\")\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for review in test[\"news\"]:\n",
    "    # Get the wordlist in each news article.\n",
    "    result = tokenizer.parse(review).replace(\"\\u3000\",\"\").replace(\"\\n\",\"\")\n",
    "    result = re.sub(r'[0123456789０１２３４５６７８９！＠＃＄％＾＆\\-|\\\\＊\\“（）＿■×※⇒—●(：〜＋=)／*&^%$#@!~`){}…\\[\\]\\\"\\'\\”:;<>?＜＞？、。・,./『』【】「」→←○]+', \"\", result)\n",
    "    h = result.split(\" \")\n",
    "    h = filter((\"\").__ne__, h)\n",
    "    words = list(h)\n",
    "    gwbowv_fasttext_test[counter] = create_cluster_vector_and_gwbowv(prob_wordvecs, words, word_centroid_map, word_centroid_prob_map, num_features, word_idf_dict, featurenames, num_clusters)\n",
    "    counter+=1\n",
    "    if counter % 1000 == 0:\n",
    "        print (\"Test News Covered : \",counter)\n",
    "\n",
    "test_gwbowv_name = \"TEST_SDV_fasttext_\" + str(num_clusters) + \"cluster_\" + str(num_features) + \"feature_matrix_gmm_sparse.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwbowv_fasttext[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Making sparse...\")\n",
    "# Set the threshold percentage for making it sparse. \n",
    "percentage = 0.04\n",
    "min_no = min_no*1.0/len(train[\"news\"])\n",
    "max_no = max_no*1.0/len(train[\"news\"])\n",
    "print (\"Average min: \", min_no)\n",
    "print (\"Average max: \", max_no)\n",
    "thres = (abs(max_no) + abs(min_no))/2\n",
    "thres = thres*percentage\n",
    "\n",
    "# Make values of matrices which are less than threshold to zero.\n",
    "temp = abs(gwbowv_fasttext) < thres\n",
    "gwbowv_fasttext[temp] = 0\n",
    "\n",
    "temp = abs(gwbowv_fasttext_test) < thres\n",
    "gwbowv_fasttext_test[temp] = 0\n",
    "\n",
    "#saving gwbowv train and test matrices\n",
    "np.save(\"../japanese-dataset/livedoor-news-corpus/model/\"+gwbowv_name, gwbowv_fasttext)\n",
    "np.save(\"../japanese-dataset/livedoor-news-corpus/model/\"+test_gwbowv_name, gwbowv_fasttext_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test lgb SCDV based fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import lightgbm as lgb\n",
    "\n",
    "start = time.time()\n",
    "clf = lgb.LGBMClassifier(objective=\"multiclass\")\n",
    "clf.fit(gwbowv_fasttext, train[\"class\"])\n",
    "Y_true, Y_pred  = test[\"class\"], clf.predict(gwbowv_fasttext_test)\n",
    "print (\"Report\")\n",
    "print (classification_report(Y_true, Y_pred, digits=6))\n",
    "print (\"Accuracy: \",clf.score(gwbowv_fasttext_test,test[\"class\"]))\n",
    "print (\"Time taken:\", time.time() - start, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
